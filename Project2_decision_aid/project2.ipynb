{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report as clr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "from  sklearn.metrics import log_loss\n",
    "# from  sklearn.metrics import neg_log_loss\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_train, path_test):\n",
    "    \"\"\"load the train and test data\n",
    "\n",
    "    Args:\n",
    "        path_train (csv file): the path of training dataset\n",
    "        path_test (csv file):  the path of testing dataset\n",
    "\n",
    "    Returns:\n",
    "        dataframe: the dataframe for training and testing dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(path_train)\n",
    "    test_data = pd.read_csv(path_test)\n",
    "\n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 pre-process train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_D(name,data):\n",
    "    \"\"\"Apply getting dummies method to change the categorical features\n",
    "\n",
    "    Args:\n",
    "        name (string): the column name of the categorical feature\n",
    "        data (dataframe): the input dataset\n",
    "\n",
    "    Returns:\n",
    "        dataframe: the dataframe after changing the categorical features\n",
    "    \"\"\"\n",
    "    res = pd.get_dummies(data[name]).rename(columns=lambda x:name+'_' +str(x))\n",
    "    data = data.join(res)\n",
    "    data.drop(columns=[name], inplace=True)\n",
    "    return data\n",
    "\n",
    "def transfrom_distance(x):\n",
    "    \"\"\"change the format of distance\n",
    "\n",
    "    Args:\n",
    "        x (float): the value of distance\n",
    "\n",
    "    Returns:\n",
    "        int: the category value of distance\n",
    "    \"\"\"\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    \n",
    "    elif x<15:\n",
    "        return 1\n",
    "    \n",
    "    else: return 2\n",
    "\n",
    "def pre_process_train(data, if_trip_class=False):\n",
    "\n",
    "    \"\"\"\n",
    "    preprocess the train data.\n",
    "    data : train_data, dataframe object\n",
    "    \"\"\"\n",
    "\n",
    "    date_time = pd.to_datetime(data['travel_date'], format='%d/%m/%Y').dt.dayofweek # change week time\n",
    "    data['travel_date'] = date_time\n",
    "\n",
    "    # get dummies of categorical column\n",
    "    names=['travel_date','survey_language','disability','o_location_type','d_location_type','res_type','rent_own','o_purpose_category','d_purpose_category','age','employment','student','planning_apps','industry','gender','education','income_aggregate']\n",
    "    for name in names:\n",
    "        data=Get_D(name,data)\n",
    "    #data.drop(columns=['travel_date'], inplace=True)\n",
    "    data.drop(columns=['id','trip_n','person_id'], inplace=True)\n",
    "\n",
    "    # change the trip_distance\n",
    "    if if_trip_class:\n",
    "        data['trip_distance'] = data['trip_distance'].apply(transfrom_distance)\n",
    "\n",
    "    search_columns = data.columns.drop(['trip_distance', 'mode'])\n",
    "\n",
    "    return search_columns, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_features(data, search_columns,num_features):\n",
    "\n",
    "    \"\"\"\n",
    "    get the most importance features for data with in search_columns\n",
    "\n",
    "    data : dataframe\n",
    "    search_columns : list contains columns in data\n",
    "    num_features : numbers (int), the feature number we need\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[search_columns]\n",
    "    y = data['trip_distance']\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X,y)\n",
    "    data_tuples = list(zip(search_columns,rf.feature_importances_))\n",
    "    df=pd.DataFrame(data_tuples, columns=['name','value'])\n",
    "    dataindex=list(df.sort_values('value',ascending=False)[0:num_features]['name'].values)\n",
    "    dataindex=pd.Index(dataindex)\n",
    "\n",
    "    return dataindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_features(data, search_columns,num_features):\n",
    "\n",
    "    \"\"\"\n",
    "    get the most importance features for data with in search_columns\n",
    "\n",
    "    data : dataframe\n",
    "    search_columns : list contains columns in data\n",
    "    num_features : numbers (int), the feature number we need\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[search_columns]\n",
    "    y = data['mode']\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X,y)\n",
    "    data_tuples = list(zip(search_columns,rf.feature_importances_))\n",
    "    df=pd.DataFrame(data_tuples, columns=['name','value'])\n",
    "    dataindex=list(df.sort_values('value',ascending=False)[0:num_features]['name'].values)\n",
    "    dataindex=pd.Index(dataindex)\n",
    "\n",
    "    return dataindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 predict travel distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(test, predict):\n",
    "    \"\"\"Apply RMSE method\n",
    "\n",
    "    Args:\n",
    "        test (array): the true value\n",
    "        predict (array): the prediction value\n",
    "\n",
    "    Returns:\n",
    "        float: the RMSE value\n",
    "    \"\"\"\n",
    "    test = np.array(test)\n",
    "    predict = np.array(predict)\n",
    "    diff = np.sqrt(((test-predict)**2).sum()/len(test))\n",
    "    return diff\n",
    "\n",
    "def class_dis_predict_rf_grid(data, search_columns_feature, rf_params, if_trip_class=False):\n",
    "\n",
    "    \"\"\"\n",
    "    predict distance with random forest and grid search\n",
    "\n",
    "    data: dataframe\n",
    "    search_columns_feature: list of columns you need for classify\n",
    "\n",
    "    best_param: the best parameters for corss validation for this classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    X = data[search_columns_feature]\n",
    "    y = data[['trip_distance','mode']]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    if if_trip_class:\n",
    "\n",
    "        clf = RandomForestClassifier(random_state=0)\n",
    "        grid = GridSearchCV(clf, rf_params, cv=5, scoring='neg_log_loss')\n",
    "        grid.fit(X_train_std,y_train['trip_distance'])\n",
    "        predict_dis = grid.predict(X_test_std)\n",
    "        predict_dis_prob = grid.predict_proba(X_test_std)\n",
    "        predict_dis_train = grid.predict(X_train_std)\n",
    "        best_param = grid.best_params_\n",
    "        # print the results\n",
    "        print('score for dis is {}'.format(log_loss(y_test['trip_distance'],predict_dis_prob)))\n",
    "    \n",
    "    else:\n",
    "\n",
    "        clf = RandomForestRegressor(random_state=0)\n",
    "        grid = GridSearchCV(clf, rf_params, cv=5)\n",
    "        grid.fit(X_train_std,y_train['trip_distance'])\n",
    "        predict_dis = grid.predict(X_test_std)\n",
    "        predict_dis_train = grid.predict(X_train_std)\n",
    "        best_param = grid.best_params_\n",
    "        print('score for dis is {}'.format(score(predict_dis, y_test['trip_distance'])))\n",
    "\n",
    "\n",
    "    return best_param, predict_dis_train, predict_dis\n",
    "\n",
    "def class_dis_predict_rf_random(data, search_columns_feature, rf_params, if_trip_class=False):\n",
    "\n",
    "    \"\"\"\n",
    "    predict distance with random forest and grid search\n",
    "\n",
    "    data: dataframe\n",
    "    search_columns_feature: list of columns you need for classify\n",
    "\n",
    "    best_param: the best parameters for corss validation for this classifier\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[search_columns_feature]\n",
    "    y = data[['trip_distance','mode']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    if if_trip_class:\n",
    "\n",
    "        clf = RandomForestClassifier()\n",
    "        Random = RandomizedSearchCV(clf, param_distributions=rf_params,cv=5,scoring='neg_log_loss',n_iter=20)\n",
    "        Random.fit(X_train_std,y_train['trip_distance'])\n",
    "        Random_best_param = Random.best_params_\n",
    "        predict_dis = Random.predict(X_test_std)\n",
    "        predict_dis_prob = Random.predict_proba(X_test_std)\n",
    "        predict_dis_train = Random.predict(X_train_std)\n",
    "        print('score for dis is {}'.format(log_loss(y_test['trip_distance'],predict_dis_prob)))\n",
    "    \n",
    "    else:\n",
    "        clf = RandomForestRegressor()\n",
    "        Random = RandomizedSearchCV(clf, param_distributions=rf_params,cv=5,n_iter=20)\n",
    "        Random.fit(X_train_std,y_train['trip_distance'])\n",
    "        Random_best_param = Random.best_params_\n",
    "        predict_dis = Random.predict(X_test_std)\n",
    "        predict_dis_train = Random.predict(X_train_std)\n",
    "        print('score for dis is {}'.format(score(predict_dis, y_test['trip_distance'])))\n",
    "\n",
    "    return Random_best_param, predict_dis_train, predict_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 predict mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_predict_rf_grid(data,search_columns_feature, rf_params, predict_train_dis, predict_dis):\n",
    "\n",
    "    \"\"\"\n",
    "    data,search_columns_pca, rf_params\n",
    "    predict_train_dis : distance of train predict by the former classifier\n",
    "    predict_dis\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[search_columns_feature]\n",
    "    y = data[['trip_distance','mode']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    X_train['dis'] = predict_train_dis\n",
    "    X_test['dis'] = predict_dis\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    grid = GridSearchCV(clf, rf_params, cv=5, scoring='neg_log_loss')\n",
    "    grid.fit(X_train_std,y_train['mode'])\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    mode_predict_prob = grid.predict_proba(X_test_std)\n",
    "    print(log_loss(y_test['mode'],mode_predict_prob))\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def mode_predict_rf_random(data,search_columns_feature, rf_params, predict_train_dis, predict_dis):\n",
    "\n",
    "    X = data[search_columns_feature]\n",
    "    y = data[['trip_distance','mode']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    X_train['dis'] = predict_train_dis\n",
    "    X_test['dis'] = predict_dis\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    # Random = RandomForestClassifier()\n",
    "    Random = RandomizedSearchCV(clf, param_distributions=rf_params,cv=5,scoring='neg_log_loss',n_iter=20)\n",
    "    Random.fit(X_train_std,y_train['mode'])\n",
    "    Random_best_param = Random.best_params_\n",
    "    mode_predict = Random.predict(X_test_std)\n",
    "    mode_predict_prob = Random.predict_proba(X_test_std)\n",
    "    print(log_loss(y_test['mode'],mode_predict_prob))\n",
    "\n",
    "\n",
    "    return Random_best_param\n",
    "\n",
    "\n",
    "def mode_predict_boost_random(data,search_columns_feature, param_boost, predict_train_dis, predict_dis):\n",
    "\n",
    "\n",
    "    X = data[search_columns_feature]\n",
    "    y = data[['trip_distance','mode']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    X_train['dis'] = predict_train_dis\n",
    "    X_test['dis'] = predict_dis\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    clf = xgb.XGBClassifier()\n",
    "\n",
    "    rs_clf = RandomizedSearchCV(clf, param_boost, n_iter=10,\n",
    "                                n_jobs=1, verbose=2, cv=5,\n",
    "                                scoring='neg_log_loss', refit=False, random_state=42)\n",
    "\n",
    "    rs_clf.fit(X_train_std,y_train['mode'])\n",
    "    XGboost_best_params = rs_clf.best_params_\n",
    "\n",
    "    clf_boost =  xgb.XGBClassifier(**XGboost_best_params)\n",
    "    clf_boost.fit(X_train_std, y_train['mode'])\n",
    "    mode_predict_prob = clf_boost.predict_proba(X_test_std)\n",
    "    print(log_loss(y_test['mode'],mode_predict_prob))\n",
    "\n",
    "    return XGboost_best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 run-all cross validation for different pca strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choosing the hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "rf_params_dis = {\n",
    "    'n_estimators': [100],\n",
    "    #'max_features': ['sqrt',0.5],\n",
    "    'min_samples_split':[2],\n",
    "    'max_depth': [4,8], # best\n",
    "    'min_samples_leaf': [2],\n",
    "    #\"bootstrap\":[True,False],\n",
    "    # \"criterion\":['gini','entropy']\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100],\n",
    "    #'max_features': ['sqrt',0.5],\n",
    "    'min_samples_split':[2],\n",
    "    'max_depth': [4,8], # best\n",
    "    'min_samples_leaf': [2],\n",
    "    #\"bootstrap\":[True,False],\n",
    "    # \"criterion\":['gini','entropy']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_boost = {'subsample': [0.9],\n",
    " 'silent': [False],\n",
    " 'reg_lambda': [10.0],\n",
    " 'n_estimators': [10,100],\n",
    " 'min_child_weight': [0.2,0.5],\n",
    " 'max_depth': [6,10,15],\n",
    " 'learning_rate': [0.001,0.1,0.2],\n",
    " 'gamma': [0.5,1],\n",
    " 'colsample_bytree': [0.4,0.6,0.8],\n",
    " 'colsample_bylevel': [0.4,0.6,0.8]}\n",
    "\n",
    "# param_boost = {\n",
    "#            'silent': [False],\n",
    "#         'max_depth': [6, 10, 15, 20],\n",
    "#         'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "#         'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#         'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#         'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#         'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "#         'gamma': [0, 0.25, 0.5, 1.0],\n",
    "#         'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "#         'n_estimators': [100]}\n",
    "\n",
    "\n",
    "def run_all_function(path_train,path_test,rf_params,num_features_dis,num_features_dis_mode,if_trip_class):\n",
    "    \"\"\"\n",
    "    This function is to run the process of evaluating, contain loading, preprocess, predict distance, predict mode\n",
    "\n",
    "    num_features : int, the number of features you want to train the classifier to predict the mode\n",
    "    \"\"\"\n",
    "\n",
    "    # get data\n",
    "    data, test_data = load_data(path_train, path_test)\n",
    "\n",
    "    # pre-process\n",
    "    search_columns, data = pre_process_train(data,if_trip_class)\n",
    "\n",
    "    # pca analysis\n",
    "\n",
    "    search_columns_feature = dis_features(data, search_columns,num_features_dis)\n",
    "    search_columns_feature_mode = mode_features(data, search_columns,num_features_dis_mode)\n",
    "\n",
    "    #delete the imbalanced data\n",
    "    search_columns_feature1=search_columns_feature.copy()\n",
    "    for i in search_columns_feature1:\n",
    "        if data[i].mean()<0.05:\n",
    "            search_columns_feature=search_columns_feature.drop(i)\n",
    "    #delete the imbalanced data\n",
    "    search_columns_feature2=search_columns_feature_mode.copy()\n",
    "    for i in search_columns_feature2:\n",
    "        if data[i].mean()<0.05:\n",
    "            search_columns_feature_mode=search_columns_feature_mode.drop(i)\n",
    "\n",
    "    dataindex1 = search_columns_feature\n",
    "    dataindex2 = search_columns_feature_mode\n",
    "    # predict distance\n",
    "    print(\"-------------Random Forest grid search for distance-----------------\")\n",
    "    best_param, predict_dis_train, predict_dis = class_dis_predict_rf_grid(data, dataindex1, rf_params,if_trip_class)\n",
    "\n",
    "    print(\"-------------Random Forest random search for distance-----------------\")\n",
    "    best_param2, predict_dis_train2, predict_dis2 = class_dis_predict_rf_random(data, dataindex1, rf_params,if_trip_class)\n",
    "    \n",
    "    # predict mode\n",
    "    print(' **************** the number of features selected for predicting mode is {} ****************'.format(num_features_dis_mode))\n",
    "    \n",
    "    print('the result for grid search random forest')\n",
    "    mode_grid_best_params = mode_predict_rf_grid(data,dataindex2, rf_params, predict_dis_train, predict_dis)\n",
    "    \n",
    "    print('the result for Random search random forest')\n",
    "    mode_Random_best_param = mode_predict_rf_random(data,dataindex2, rf_params, predict_dis_train, predict_dis)\n",
    "\n",
    "    print('the result for random XGBoost')\n",
    "    XGboost_best_params = mode_predict_boost_random(data,dataindex2, param_boost, predict_dis_train, predict_dis)\n",
    "\n",
    "    return best_param,best_param2,mode_grid_best_params,mode_Random_best_param,XGboost_best_params, search_columns_feature,search_columns_feature_mode,data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection and distance prediction model choice\n",
    "if_trip_class=[False]\n",
    "num_features_dis=[60]\n",
    "num_features_dis_mode=[60,80,100,136]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Random Forest grid search for distance-----------------\n",
      "score for dis is 3.2979333336043135\n",
      "-------------Random Forest random search for distance-----------------\n",
      "score for dis is 3.2979954498376407\n",
      " **************** the number of features selected for predicting mode is 100 ****************\n",
      "the result for grid search random forest\n",
      "1.109297200456094\n",
      "the result for Random search random forest\n",
      "1.1118921775210997\n",
      "the result for random XGBoost\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[18:26:35] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:26:35] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  38.6s\n",
      "[18:27:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:27:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  49.2s\n",
      "[18:28:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  50.3s\n",
      "[18:28:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  58.0s\n",
      "[18:29:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:29:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time= 1.1min\n",
      "[18:30:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:30:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.001, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.7s\n",
      "[18:30:57] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:30:57] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.001, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.6s\n",
      "[18:30:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:30:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.001, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.8s\n",
      "[18:31:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.001, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   2.0s\n",
      "[18:31:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.001, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.7s\n",
      "[18:31:04] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  29.4s\n",
      "[18:31:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  29.1s\n",
      "[18:32:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:32:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  33.6s\n",
      "[18:32:36] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:32:37] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  32.7s\n",
      "[18:33:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:33:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  34.8s\n",
      "[18:33:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:33:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8, gamma=1, learning_rate=0.2, max_depth=15, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time= 1.4min\n",
      "[18:35:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:35:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8, gamma=1, learning_rate=0.2, max_depth=15, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time= 1.1min\n",
      "[18:36:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:36:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8, gamma=1, learning_rate=0.2, max_depth=15, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  54.7s\n",
      "[18:37:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:37:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8, gamma=1, learning_rate=0.2, max_depth=15, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  57.7s\n",
      "[18:38:06] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:38:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.8, gamma=1, learning_rate=0.2, max_depth=15, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time= 1.1min\n",
      "[18:39:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=0.5, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.6s\n",
      "[18:39:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=0.5, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.8s\n",
      "[18:39:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=0.5, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.7s\n",
      "[18:39:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=0.5, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.2s\n",
      "[18:39:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=0.5, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   1.4s\n",
      "[18:39:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   3.3s\n",
      "[18:39:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:22] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   3.8s\n",
      "[18:39:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   4.0s\n",
      "[18:39:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   3.9s\n",
      "[18:39:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.8, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=10, min_child_weight=0.2, n_estimators=10, reg_lambda=10.0, silent=False, subsample=0.9; total time=   4.0s\n",
      "[18:39:37] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:37] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.8, gamma=1, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  15.6s\n",
      "[18:39:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:39:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.8, gamma=1, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  14.3s\n",
      "[18:40:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:40:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.8, gamma=1, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  18.9s\n",
      "[18:40:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:40:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.8, gamma=1, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  19.4s\n",
      "[18:40:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:40:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.8, gamma=1, learning_rate=0.001, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  22.5s\n",
      "[18:41:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:41:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time= 1.0min\n",
      "[18:42:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:42:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  42.2s\n",
      "[18:42:52] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:42:52] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  38.4s\n",
      "[18:43:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:43:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  39.1s\n",
      "[18:44:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:44:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.4, colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_depth=15, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  31.2s\n",
      "[18:44:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:44:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=6, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=   9.9s\n",
      "[18:44:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:44:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=6, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  11.1s\n",
      "[18:45:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:45:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=6, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  12.2s\n",
      "[18:45:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:45:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=6, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  12.1s\n",
      "[18:45:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:45:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=0.5, learning_rate=0.1, max_depth=6, min_child_weight=0.5, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  13.3s\n",
      "[18:45:40] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:45:40] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=1, learning_rate=0.2, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  13.0s\n",
      "[18:45:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:45:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=1, learning_rate=0.2, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  12.3s\n",
      "[18:46:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:46:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=1, learning_rate=0.2, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  12.4s\n",
      "[18:46:17] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:46:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=1, learning_rate=0.2, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  13.7s\n",
      "[18:46:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:46:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END colsample_bylevel=0.6, colsample_bytree=0.4, gamma=1, learning_rate=0.2, max_depth=6, min_child_weight=0.2, n_estimators=100, reg_lambda=10.0, silent=False, subsample=0.9; total time=  14.8s\n",
      "[18:46:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:46:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7368675713759341\n"
     ]
    }
   ],
   "source": [
    "for itc in if_trip_class:\n",
    "    for nfd in num_features_dis:\n",
    "        for nfdm in num_features_dis_mode:\n",
    "            best_param,best_param2,mode_grid_best_params,mode_Random_best_param,XGboost_best_params,search_columns_feature,search_columns_feature_mode,data=run_all_function('nyc_train_validate.csv','nyc_test.csv',rf_params,nfd,nfdm,itc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('nyc_test.csv')\n",
    "def prediction(test_data, classifier_params_dis, classifier_params_mode_rf, search_columns_feature,search_columns_feature_mode):\n",
    "    \n",
    "\n",
    "    # prepare the data\n",
    "    date_time = pd.to_datetime(test_data['travel_date'], format='%d/%m/%Y').dt.dayofweek\n",
    "    test_data['travel_date'] = date_time\n",
    "    names=['travel_date','survey_language','disability','o_location_type','d_location_type','res_type','rent_own','o_purpose_category','d_purpose_category','age','employment','student','planning_apps','industry','gender','education','income_aggregate']\n",
    "    for name in names:\n",
    "        test_data=Get_D(name,test_data)\n",
    "    test_data.drop(columns=['id','trip_n','person_id'], inplace=True)\n",
    "\n",
    "\n",
    "    # predict the dis\n",
    "    rf=RandomForestRegressor(**classifier_params_dis) \n",
    "    x_train_dis= data[search_columns_feature]\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(x_train_dis)\n",
    "    x_train_dis = sc.transform(x_train_dis)\n",
    "    test_data_std = sc.transform(test_data[search_columns_feature])\n",
    "\n",
    "    rf.fit(x_train_dis,data['trip_distance'])\n",
    "    predict_dis = rf.predict(test_data_std)\n",
    "    # test_data['dis'] = predict_dis\n",
    "    x_train_mode = data[search_columns_feature_mode]\n",
    "    x_train_mode['dis']=rf.predict(x_train_dis)\n",
    "    # predict the mode\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(x_train_mode)\n",
    "    test_data1=test_data[search_columns_feature_mode]\n",
    "    test_data1['dis']=predict_dis\n",
    "    x_train_mode = sc.transform(x_train_mode)\n",
    "    Test_data_std = sc.transform(test_data1)\n",
    "    print(Test_data_std.shape)\n",
    "\n",
    "\n",
    "    clf1 = xgb.XGBClassifier(**XGboost_best_params)\n",
    "    clf2=RandomForestClassifier(**classifier_params_mode_rf)\n",
    "    clf1.fit(x_train_mode,data['mode'])\n",
    "    clf2.fit(x_train_mode,data['mode'])\n",
    "    mode_predict_rf = clf2.predict_proba(Test_data_std)\n",
    "    mode_predict_xgb = clf1.predict_proba(Test_data_std)\n",
    "\n",
    "    return mode_predict_rf, mode_predict_xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26294, 94)\n",
      "[18:07:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:07:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "mode_predict_rf,mode_predict_xgb = prediction(test_data, best_param, mode_Random_best_param, search_columns_feature,search_columns_feature_mode)\n",
    "\n",
    "#  save prediction\n",
    "result=pd.DataFrame(data=mode_predict_xgb,columns=['bike', 'bus', 'drive', 'other', 'passenger', 'subway', 'walk'])\n",
    "test_data = pd.read_csv('nyc_test.csv')\n",
    "result['id']=test_data['id']\n",
    "result=result[['id','drive','passenger','bus','subway','bike','walk','other']]\n",
    "result=result.set_index('id')\n",
    "result.to_csv('test_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction\n",
    "\n",
    "result=pd.DataFrame(data=mode_predict_rf,columns=['bike', 'bus', 'drive', 'other', 'passenger', 'subway', 'walk'])\n",
    "test_data = pd.read_csv('nyc_test.csv')\n",
    "result['id']=test_data['id']\n",
    "result=result[['id','drive','passenger','bus','subway','bike','walk','other']]\n",
    "result=result.set_index('id')\n",
    "result.to_csv('test_rf.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e407f4f5fdc20601611945a3f27d54c215ab15f552e586915410c08bf73f76"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('qingjun')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
